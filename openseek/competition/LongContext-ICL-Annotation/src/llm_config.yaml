serve:
- serve_id: vllm_model
  engine: vllm
  engine_args:
    model: ../Qwen3-4B
    host: 0.0.0.0
    uvicorn_log_level: warning
    port: 2026
    gpu_memory_utilization: 0.9
    trust_remote_code: true
    no_enable_prefix_caching: true

experiment:
  exp_name: qwen3_4b
  exp_dir: outputs/${experiment.exp_name}
  task:
    type: serve
  runner:
    hostfile: null
    deploy:
      use_fs_serve: false
  envs:
    CUDA_VISIBLE_DEVICES: 0
    CUDA_DEVICE_MAX_CONNECTIONS: 1

action: run

hydra:
  run:
    dir: ${experiment.exp_dir}/hydra